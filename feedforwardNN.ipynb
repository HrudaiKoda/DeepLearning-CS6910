{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "Ei8zzE07CR9V"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set(y_train)\n",
        "variousSamples = list()\n",
        "for i in classes:\n",
        "  ind = np.where(y_train == i)[0][0]\n",
        "  variousSamples.append(x_train[ind])"
      ],
      "metadata": {
        "id": "ZMv-1-gtCi6p"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_flatten_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)\n",
        "y_encoded = np.zeros((y_train.shape[0], max(classes) + 1))\n",
        "y_encoded[np.arange(y_train.shape[0]), y_train] = 1"
      ],
      "metadata": {
        "id": "8nEmrnfUFPAn"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded = y_encoded.reshape(60000,10,1)"
      ],
      "metadata": {
        "id": "ldJvYFLriFPn"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(x):\n",
        "  x_norm = x.astype('float32')\n",
        "  x_norm = x_norm / 255.0\n",
        "  return x_norm\n",
        "\n",
        "x_flatten_train = normalize_data(x_flatten_train)"
      ],
      "metadata": {
        "id": "BJGaU2CWmYB4"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = 3\n",
        "samples = y_train.shape[0]\n",
        "lr = 0.001\n",
        "epochs = 5\n",
        "nodesPerLayer = list()"
      ],
      "metadata": {
        "id": "OjkOPsC-IDZm"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodesPerLayer.append(784)\n",
        "for i in range(0,layers):\n",
        "  nodesPerLayer.append(int(1024/(2**(i+1))))\n",
        "nodesPerLayer.append(10)"
      ],
      "metadata": {
        "id": "lXxQd9FhK_ax"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(activation,a_k):\n",
        "  if(activation == \"tanh\"):\n",
        "    a_k = np.tanh(a_k)\n",
        "  else:\n",
        "    a_k = 1/(1 + np.exp(-1*a_k))\n",
        "  return a_k"
      ],
      "metadata": {
        "id": "pR0k8a2uXy3f"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivativeFun(activation,a_k):\n",
        "  activationResult = func(activation,a_k)\n",
        "  if(activation == \"tanh\"):\n",
        "    activationResult = 1 - (activationResult**2)\n",
        "  else:\n",
        "    activationResult = activationResult - (activationResult**2)\n",
        "\n",
        "  return activationResult"
      ],
      "metadata": {
        "id": "Uas-jhRzYIOm"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision(a_k,classificationFunction):\n",
        "  if classificationFunction == \"crossEntropy\":\n",
        "    a_k = np.exp(a_k - np.max(a_k))\n",
        "    a_k = a_k / sum(a_k)\n",
        "  return a_k"
      ],
      "metadata": {
        "id": "u3_w6ojmaD52"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OneHotEncode(C):\n",
        "  oneHot = np.zeros(C.shape)\n",
        "  oneHot[np.argmax(C)] = 1\n",
        "  return oneHot"
      ],
      "metadata": {
        "id": "Tn76NXflcV_N"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardProp(inputX,activation,classificationFunction,weights,bias):\n",
        "  h_k = inputX\n",
        "  PreActivations = list()\n",
        "  PostActivations = list()\n",
        "  PostActivations.append(h_k)\n",
        "  for k in range(0,layers):\n",
        "    a_k = bias[k] + np.matmul(weights[k],h_k)\n",
        "    PreActivations.append(a_k)\n",
        "    h_k = func(activation,a_k)\n",
        "    PostActivations.append(h_k)\n",
        "  a_k = bias[layers] + np.matmul(weights[layers],h_k)\n",
        "  PreActivations.append(a_k)\n",
        "  yPred = decision(a_k,classificationFunction)\n",
        "  return PreActivations,PostActivations,yPred"
      ],
      "metadata": {
        "id": "ErGdug_sVCXM"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backProp(real, pred, h_k, weights, activation, PreActivations):\n",
        "    a_l_L_theta = pred - real\n",
        "    currentActivationGradient = a_l_L_theta\n",
        "    WeightGradients = []\n",
        "    biasGradients = []\n",
        "    layers = len(weights) - 1\n",
        "\n",
        "    for i in range(layers, -1, -1):\n",
        "        W_i_L_theta = currentActivationGradient*np.transpose(h_k[i])\n",
        "        WeightGradients.insert(0, W_i_L_theta)\n",
        "        b_i_L_theta = np.sum(currentActivationGradient, axis=0, keepdims=True)\n",
        "        biasGradients.insert(0, b_i_L_theta)\n",
        "\n",
        "        if i > 0:\n",
        "            h_i_prev_L_theta = np.matmul(weights[i].T, currentActivationGradient)\n",
        "            currentActivationGradient = h_i_prev_L_theta * derivativeFun(activation, PreActivations[i - 1])\n",
        "\n",
        "    return WeightGradients, biasGradients\n"
      ],
      "metadata": {
        "id": "d7ZFrgcUSaHK"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = list()\n",
        "bias = list()\n",
        "for i in range(1,len(nodesPerLayer)):\n",
        "  #*(np.sqrt(0.001/(nodesPerLayer[i]+nodesPerLayer[i-1])))\n",
        "  w = np.random.randn(nodesPerLayer[i],nodesPerLayer[i-1])*0.1\n",
        "  b =  np.random.randn(nodesPerLayer[i],1)\n",
        "  #np.random.rand(nodesPerLayer[i],1)\n",
        "  weights.append(w)\n",
        "  bias.append(b)\n",
        "\n",
        "for i in range(0,epochs):\n",
        "  for j in range(0,samples):\n",
        "    A,B,C = forwardProp(x_flatten_train[j],\"sigmoid\",\"crossEntropy\",weights,bias)\n",
        "    #print(x_flatten_train)\n",
        "    newC = OneHotEncode(C)\n",
        "    #print(A[0][0])\n",
        "    Wdelta,Bdelta = backProp(y_encoded[j],C,B,weights,\"sigmoid\",A)\n",
        "    #print(Wdelta[1])\n",
        "    #print(\"#########\")\n",
        "    for k in range(0,len(weights)):\n",
        "      weights[k] = weights[k] - lr*Wdelta[k]\n",
        "      bias[k] = bias[k] - lr*Bdelta[k]\n",
        "    if j%600 == 0 :\n",
        "      print(j/600)"
      ],
      "metadata": {
        "id": "Ns8hWfTZyvZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(weights,bias,x_test,y_test):\n",
        "  count = 0\n",
        "  for i in range(0,x_test.shape[0]):\n",
        "    A,B,C = forwardProp(x_test[i],\"logistic\",\"crossEntropy\",weights,bias)\n",
        "    #print(C)\n",
        "    if( y_test[i] == np.argmax(C)):\n",
        "      count+=1\n",
        "  print(\"Accuracy :\" + str((count/y_test.shape[0])*100) + \"%\")"
      ],
      "metadata": {
        "id": "lU8sJHCOUnqN"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_flatten_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2],1)\n",
        "x_flatten_test = normalize_data(x_flatten_test)"
      ],
      "metadata": {
        "id": "4rmZko4FZ94n"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testModel(weights,bias,x_flatten_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V94N6E6_Z94o",
        "outputId": "38bc199d-5389-49ec-9c44-4f18744caeaa"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :64.8%\n"
          ]
        }
      ]
    }
  ]
}