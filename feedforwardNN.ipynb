{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix as cnfm\n",
        "import itertools\n",
        "import math\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(project='Assignment 1', entity='CS23M028')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "IASdQzqcXe89",
        "outputId": "132663f2-d486-451d-8468-ba63c3aa7453"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240316_190436-81bgkxgl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m028/Assignment%201/runs/81bgkxgl' target=\"_blank\">glorious-silence-1</a></strong> to <a href='https://wandb.ai/cs23m028/Assignment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m028/Assignment%201' target=\"_blank\">https://wandb.ai/cs23m028/Assignment%201</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m028/Assignment%201/runs/81bgkxgl' target=\"_blank\">https://wandb.ai/cs23m028/Assignment%201/runs/81bgkxgl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs23m028/Assignment%201/runs/81bgkxgl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7856a0838490>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ei8zzE07CR9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1927c58-9997-4026-ba9c-055bede8a19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "BJGaU2CWmYB4"
      },
      "outputs": [],
      "source": [
        "def normalize_data(x):\n",
        "  x_norm = x.astype('float32')\n",
        "  x_norm = x_norm / 255.0\n",
        "  return x_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pR0k8a2uXy3f"
      },
      "outputs": [],
      "source": [
        "def func(activation,a_k):\n",
        "  if(activation == \"tanh\"):\n",
        "    a_k = np.tanh(a_k)\n",
        "  elif(activation == \"sigmoid\"):\n",
        "    a_k = 1/(1 + np.exp(-1*a_k))\n",
        "  return a_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Uas-jhRzYIOm"
      },
      "outputs": [],
      "source": [
        "def derivativeFun(activation,a_k):\n",
        "  activationResult = func(activation,a_k)\n",
        "  if(activation == \"tanh\"):\n",
        "    activationResult = 1 - (activationResult**2)\n",
        "  elif(activation == \"sigmoid\"):\n",
        "    activationResult = activationResult - (activationResult**2)\n",
        "\n",
        "  return activationResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "u3_w6ojmaD52"
      },
      "outputs": [],
      "source": [
        "def decision(a_k,classificationFunction):\n",
        "  if classificationFunction == \"crossEntropy\":\n",
        "    a_k = np.exp(a_k - np.max(a_k))\n",
        "    a_k = a_k / sum(a_k)\n",
        "  return a_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ErGdug_sVCXM"
      },
      "outputs": [],
      "source": [
        "def forwardProp(inputX,activation,classificationFunction,weights,bias):\n",
        "  h_k = inputX\n",
        "  PreActivations = list()\n",
        "  PostActivations = list()\n",
        "  PostActivations.append(h_k)\n",
        "  layers = len(weights) - 1\n",
        "  for k in range(0,layers):\n",
        "    a_k = bias[k] + np.dot(weights[k],h_k)\n",
        "    PreActivations.append(a_k)\n",
        "    h_k = func(activation,a_k)\n",
        "    PostActivations.append(h_k)\n",
        "  a_k = bias[layers] + np.matmul(weights[layers],h_k)\n",
        "  PreActivations.append(a_k)\n",
        "  yPred = decision(a_k,classificationFunction)\n",
        "  return PreActivations,PostActivations,yPred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "d7ZFrgcUSaHK"
      },
      "outputs": [],
      "source": [
        "def backProp(real, pred, h_k, weights, activation, PreActivations):\n",
        "    a_l_L_theta = pred - real\n",
        "    currentActivationGradient = a_l_L_theta\n",
        "    WeightGradients = []\n",
        "    biasGradients = []\n",
        "    layers = len(weights) - 1\n",
        "\n",
        "    for i in range(layers, -1, -1):\n",
        "        W_i_L_theta = currentActivationGradient*np.transpose(h_k[i])\n",
        "        WeightGradients.insert(0, W_i_L_theta)\n",
        "        b_i_L_theta = np.sum(currentActivationGradient, axis=0, keepdims=True)\n",
        "        biasGradients.insert(0, b_i_L_theta)\n",
        "\n",
        "        if i > 0:\n",
        "            h_i_prev_L_theta = np.matmul(weights[i].T, currentActivationGradient)\n",
        "            currentActivationGradient = h_i_prev_L_theta * derivativeFun(activation, PreActivations[i - 1])\n",
        "\n",
        "    return WeightGradients, biasGradients\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, batch_size,activationFunc,epochs,lr):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", weights, bias)\n",
        "                Wdelta, Bdelta = backProp(batch_y[j], C, B, weights, activationFunc, A)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += Wdelta[k]\n",
        "                    batch_Bdelta[k] += Bdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                weights[k] -= lr * (batch_Wdelta[k] / batch_size)\n",
        "                bias[k] -= lr * (batch_Bdelta[k] / batch_size)\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "sXsdaBX9JjkV"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, gamma, batch_size,activationFunc,epochs,lr):\n",
        "\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    Wdelta = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    Bdelta = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights,activationFunc, A)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += CurrWdelta[k]\n",
        "                    batch_Bdelta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                Wdelta[k] = gamma * Wdelta[k] + lr * batch_Wdelta[k] / batch_size\n",
        "                Bdelta[k] = gamma * Bdelta[k] + lr * batch_Bdelta[k] / batch_size\n",
        "\n",
        "                weights[k] -= Wdelta[k]\n",
        "                bias[k] -= Bdelta[k]\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "Yi4rekLRNIH4"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,gamma, batch_size,activationFunc,epochs,lr):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            lookahead_weights = [w - gamma * dw for w, dw in zip(weights, weights)]\n",
        "            lookahead_bias = [b - gamma * db for b, db in zip(bias, bias)]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", lookahead_weights, lookahead_bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, lookahead_weights, activationFunc, A)\n",
        "\n",
        "                for k in range(len(weights)):\n",
        "                    weights[k] -= lr * CurrWdelta[k]\n",
        "                    bias[k] -= lr * CurrBdelta[k]\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "dUnrGa1nBWyx"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(nodesPerLayer, x_flatten_train, y_encoded, beta, eps, epochs, batch_size, lr,activationFunc):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    rmsweights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    rmsbias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                rmsweights[k] = beta * rmsweights[k] + (1 - beta) * (batch_w_delta[k] ** 2)\n",
        "                rmsbias[k] = beta * rmsbias[k] + (1 - beta) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                weights[k] -= (lr * batch_w_delta[k]) / (np.sqrt(rmsweights[k]) + eps)\n",
        "                bias[k] -= (lr * batch_b_delta[k]) / (np.sqrt(rmsbias[k]) + eps)\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "g17vSi0AT2iC"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] -= (lr * m_weights_hat) / (np.sqrt(v_weights_hat) + eps)\n",
        "                bias[k] -= (lr * m_bias_hat) / (np.sqrt(v_bias_hat) + eps)\n",
        "\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val)\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "Dha24Dxz6Wiv"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Nadam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,20):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] -= lr * (beta1 * m_weights_hat + (1 - beta1) * batch_w_delta[k]) / (np.sqrt(v_weights_hat) + eps)\n",
        "                bias[k] -= lr * (beta1 * m_bias_hat + (1 - beta1) * batch_b_delta[k]) / (np.sqrt(v_bias_hat) + eps)\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "kFFut1Z984-Z"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,activationFunc,epochs,lr):\n",
        "  weights = list()\n",
        "  bias = list()\n",
        "  for i in range(1,len(nodesPerLayer)):\n",
        "    w = np.random.randn(nodesPerLayer[i],nodesPerLayer[i-1])*0.1\n",
        "    b =  np.random.randn(nodesPerLayer[i],1)\n",
        "    weights.append(w)\n",
        "    bias.append(b)\n",
        "  Wdelta = list()\n",
        "  Bdelta = list()\n",
        "  for i in range(0,epochs):\n",
        "    Wdelta.clear()\n",
        "    Bdelta.clear()\n",
        "    for j in range(0,len(y_encoded)):\n",
        "      A,B,C = forwardProp(x_flatten_train[j],activationFunc,\"crossEntropy\",weights,bias)\n",
        "      CurrWdelta,CurrBdelta = backProp(y_encoded[j],C,B,weights,activationFunc,A)\n",
        "      if( len(Wdelta) == 0):\n",
        "        Wdelta =  copy.deepcopy(CurrWdelta)\n",
        "        Bdelta = copy.deepcopy(CurrBdelta)\n",
        "      else:\n",
        "        for k in range(0,len(Wdelta)):\n",
        "          Wdelta[k] = Wdelta[k] + CurrWdelta[k]\n",
        "          Bdelta[k] = Bdelta[k] + CurrBdelta[k]\n",
        "      if(j%1000 == 0):\n",
        "        print(j/1000)\n",
        "    for k in range(0,len(weights)):\n",
        "      weights[k] = weights[k] - lr*Wdelta[k]\n",
        "      bias[k] = bias[k] - lr*Bdelta[k]\n",
        "  return weights,bias"
      ],
      "metadata": {
        "id": "XrUyqtbh8zlJ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def executeTraining(config,x_train,y_train):\n",
        "  FinalWeights = list()\n",
        "  FinalBias = list()\n",
        "  beta1 = 0.89\n",
        "  beta2 = 0.95\n",
        "  eps = 1e-6\n",
        "  batch_size = config.batch_size\n",
        "  layers = config.number_hidden\n",
        "  lr = config.learning_rate\n",
        "  epochs = config.epochs\n",
        "  activationFunc = config.activation\n",
        "  optimizer = config.optimizer\n",
        "\n",
        "  nodesPerLayer = list()\n",
        "  nodesPerLayer.append(784)\n",
        "  for i in range(0,layers):\n",
        "    nodesPerLayer.append(config.hidden_inputsize)\n",
        "  nodesPerLayer.append(10)\n",
        "\n",
        "  gamma = 0.6\n",
        "  betarms = 0.6\n",
        "  if(optimizer == \"gradient_descent\"):\n",
        "    FinalWeights, FinalBias = gradient_descent(nodesPerLayer,x_train,y_train,activationFunc,epochs,lr)\n",
        "  elif(optimizer == \"SGD\"):\n",
        "    FinalWeights, FinalBias = stochastic_gradient_descent(nodesPerLayer,x_train,y_train,batch_size,activationFunc,epochs,lr)\n",
        "  elif(optimizer == \"momentumGD\"):\n",
        "    FinalWeights, FinalBias = momentum_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr)\n",
        "  elif(optimizer == \"nesterovGD\"):\n",
        "    FinalWeights, FinalBias = nesterov_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr)\n",
        "  elif(optimizer == \"rmsprop\"):\n",
        "    FinalWeights, FinalBias =rmsprop(nodesPerLayer,x_train,y_train,betarms,eps,epochs,batch_size,lr,activationFunc)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    FinalWeights, FinalBias = adam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    FinalWeights, FinalBias = nadam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs)\n",
        "  return FinalWeights,FinalBias"
      ],
      "metadata": {
        "id": "WLkW8suR9RCp"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lU8sJHCOUnqN"
      },
      "outputs": [],
      "source": [
        "def testModel(weights,bias,x_test,y_test):\n",
        "  count = 0\n",
        "  loss = 0.0\n",
        "  for i in range(0,x_test.shape[0]):\n",
        "    A,B,C = forwardProp(x_test[i],\"tanh\",\"crossEntropy\",weights,bias)\n",
        "    if( y_test[i] == np.argmax(C)):\n",
        "      count+=1\n",
        "    loss += -np.sum(y_test[i] * np.log(C))\n",
        "  acc = (count/y_test.shape[0])\n",
        "  return acc,loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8nEmrnfUFPAn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load dataset\n",
        "def load_data():\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  x_flatten_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)\n",
        "  y_encoded = np.zeros((y_train.shape[0], max(classes) + 1))\n",
        "  y_encoded[np.arange(y_train.shape[0]), y_train] = 1\n",
        "  y_encoded = y_encoded.reshape(y_train.shape[0],10,1)\n",
        "  x_flatten_train = normalize_data(x_flatten_train)\n",
        "\n",
        "  x_flatten_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2],1)\n",
        "  x_flatten_test = normalize_data(x_flatten_test)\n",
        "\n",
        "  y_flatten_test = np.zeros((y_test.shape[0], max(classes) + 1))\n",
        "  y_flatten_test[np.arange(y_test.shape[0]), y_test] = 1\n",
        "  y_flatten_test = y_flatten_test.reshape(y_test.shape[0],10,1)\n",
        "\n",
        "  val_size = int(x_train.shape[0] * 0.1)\n",
        "\n",
        "  x_val = x_flatten_train[:val_size]\n",
        "  y_val = y_encoded[:val_size]\n",
        "\n",
        "  x_new_train = x_flatten_train[val_size:]\n",
        "  y_new_train = y_encoded[val_size:]\n",
        "\n",
        "  return x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_flatten_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'name':\"my-sweep-mnist\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10] #number of epochs\n",
        "        },\n",
        "        'number_hidden': {\n",
        "            'values': [5] #number of hidden layers\n",
        "        },\n",
        "        'hidden_inputsize': {\n",
        "            'values':[64, 128] #size of every hidden layer\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0] #L2 regularisation\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nadam']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[64]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values':['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu']\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"CS23M028\", project=\"Assignment 1\")"
      ],
      "metadata": {
        "id": "KJtuFndZ_QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitModel():\n",
        "  with wandb.init() as run:\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_inputsize)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation\n",
        "    np.random.seed(1)\n",
        "\n",
        "    x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_flatten_test = load_data()\n",
        "    trainedWeights,trainedBias = executeTraining(config,x_new_train,y_new_train)\n",
        "\n",
        "\n",
        "\n",
        "wandb.agent(sweep_id,executeTraining,entity=\"CS23M028\", project=\"Assignment 1\")"
      ],
      "metadata": {
        "id": "NiAR-tqwjweB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMv-1-gtCi6p"
      },
      "outputs": [],
      "source": [
        "classes = set(y_train)\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "variousSamples = list()\n",
        "for i in classes:\n",
        "  ind = np.where(y_train == i)[0][0]\n",
        "  variousSamples.append(wandb.Image(x_train[ind],caption = names[i]))\n",
        "\n",
        "wandb.log({\"examples\": variousSamples})  #logs images to the wandb panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "4rmZko4FZ94n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V94N6E6_Z94o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}