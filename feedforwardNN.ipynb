{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Ei8zzE07CR9V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import copy\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZMv-1-gtCi6p"
      },
      "outputs": [],
      "source": [
        "classes = set(y_train)\n",
        "variousSamples = list()\n",
        "for i in classes:\n",
        "  ind = np.where(y_train == i)[0][0]\n",
        "  variousSamples.append(x_train[ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "8nEmrnfUFPAn"
      },
      "outputs": [],
      "source": [
        "x_flatten_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)\n",
        "y_encoded = np.zeros((y_train.shape[0], max(classes) + 1))\n",
        "y_encoded[np.arange(y_train.shape[0]), y_train] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ldJvYFLriFPn"
      },
      "outputs": [],
      "source": [
        "y_encoded = y_encoded.reshape(60000,10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "BJGaU2CWmYB4"
      },
      "outputs": [],
      "source": [
        "def normalize_data(x):\n",
        "  x_norm = x.astype('float32')\n",
        "  x_norm = x_norm / 255.0\n",
        "  return x_norm\n",
        "\n",
        "x_flatten_train = normalize_data(x_flatten_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "OjkOPsC-IDZm"
      },
      "outputs": [],
      "source": [
        "layers = 3\n",
        "samples = y_train.shape[0]\n",
        "lr = 0.01\n",
        "epochs = 2\n",
        "nodesPerLayer = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "lXxQd9FhK_ax"
      },
      "outputs": [],
      "source": [
        "nodesPerLayer.append(784)\n",
        "for i in range(0,layers):\n",
        "  nodesPerLayer.append(int(1024/(2**(i+1))))\n",
        "nodesPerLayer.append(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "pR0k8a2uXy3f"
      },
      "outputs": [],
      "source": [
        "def func(activation,a_k):\n",
        "  if(activation == \"tanh\"):\n",
        "    a_k = np.tanh(a_k)\n",
        "  else:\n",
        "    #print(a_k)\n",
        "    a_k = 1/(1 + np.exp(-1*a_k))\n",
        "  return a_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Uas-jhRzYIOm"
      },
      "outputs": [],
      "source": [
        "def derivativeFun(activation,a_k):\n",
        "  activationResult = func(activation,a_k)\n",
        "  if(activation == \"tanh\"):\n",
        "    activationResult = 1 - (activationResult**2)\n",
        "  else:\n",
        "    activationResult = activationResult - (activationResult**2)\n",
        "\n",
        "  return activationResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "u3_w6ojmaD52"
      },
      "outputs": [],
      "source": [
        "def decision(a_k,classificationFunction):\n",
        "  if classificationFunction == \"crossEntropy\":\n",
        "    a_k = np.exp(a_k - np.max(a_k))\n",
        "    a_k = a_k / sum(a_k)\n",
        "  return a_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Tn76NXflcV_N"
      },
      "outputs": [],
      "source": [
        "def OneHotEncode(C):\n",
        "  oneHot = np.zeros(C.shape)\n",
        "  oneHot[np.argmax(C)] = 1\n",
        "  return oneHot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ErGdug_sVCXM"
      },
      "outputs": [],
      "source": [
        "def forwardProp(inputX,activation,classificationFunction,weights,bias):\n",
        "  h_k = inputX\n",
        "  PreActivations = list()\n",
        "  PostActivations = list()\n",
        "  PostActivations.append(h_k)\n",
        "  for k in range(0,layers):\n",
        "    #print(weights[k].shape,h_k.shape)\n",
        "    a_k = bias[k] + np.dot(weights[k],h_k)\n",
        "    PreActivations.append(a_k)\n",
        "    h_k = func(activation,a_k)\n",
        "    PostActivations.append(h_k)\n",
        "  a_k = bias[layers] + np.matmul(weights[layers],h_k)\n",
        "  PreActivations.append(a_k)\n",
        "  yPred = decision(a_k,classificationFunction)\n",
        "  return PreActivations,PostActivations,yPred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "d7ZFrgcUSaHK"
      },
      "outputs": [],
      "source": [
        "def backProp(real, pred, h_k, weights, activation, PreActivations):\n",
        "    a_l_L_theta = pred - real\n",
        "    currentActivationGradient = a_l_L_theta\n",
        "    WeightGradients = []\n",
        "    biasGradients = []\n",
        "    layers = len(weights) - 1\n",
        "\n",
        "    for i in range(layers, -1, -1):\n",
        "        W_i_L_theta = currentActivationGradient*np.transpose(h_k[i])\n",
        "        WeightGradients.insert(0, W_i_L_theta)\n",
        "        b_i_L_theta = np.sum(currentActivationGradient, axis=0, keepdims=True)\n",
        "        biasGradients.insert(0, b_i_L_theta)\n",
        "\n",
        "        if i > 0:\n",
        "            h_i_prev_L_theta = np.matmul(weights[i].T, currentActivationGradient)\n",
        "            currentActivationGradient = h_i_prev_L_theta * derivativeFun(activation, PreActivations[i - 1])\n",
        "\n",
        "    return WeightGradients, biasGradients\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, batch_size):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,20):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"sigmoid\", \"crossEntropy\", weights, bias)\n",
        "                Wdelta, Bdelta = backProp(batch_y[j], C, B, weights, \"sigmoid\", A)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += Wdelta[k]\n",
        "                    batch_Bdelta[k] += Bdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                weights[k] -= lr * (batch_Wdelta[k] / batch_size)\n",
        "                bias[k] -= lr * (batch_Bdelta[k] / batch_size)\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "sXsdaBX9JjkV"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, gamma, batch_size):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize momentum parameters\n",
        "    Wdelta = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    Bdelta = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"sigmoid\", \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, \"sigmoid\", A)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += CurrWdelta[k]\n",
        "                    batch_Bdelta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                Wdelta[k] = gamma * Wdelta[k] + lr * batch_Wdelta[k] / batch_size\n",
        "                Bdelta[k] = gamma * Bdelta[k] + lr * batch_Bdelta[k] / batch_size\n",
        "\n",
        "                weights[k] -= Wdelta[k]\n",
        "                bias[k] -= Bdelta[k]\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "Yi4rekLRNIH4"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,gamma, batch_size):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            lookahead_weights = [w - gamma * dw for w, dw in zip(weights, weights)]\n",
        "            lookahead_bias = [b - gamma * db for b, db in zip(bias, bias)]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"sigmoid\", \"crossEntropy\", lookahead_weights, lookahead_bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, lookahead_weights, \"sigmoid\", A)\n",
        "\n",
        "                for k in range(len(weights)):\n",
        "                    weights[k] -= lr * CurrWdelta[k]\n",
        "                    bias[k] -= lr * CurrBdelta[k]\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "dUnrGa1nBWyx"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(nodesPerLayer, x_flatten_train, y_encoded, beta, eps, epochs, batch_size= 200, lr=0.001):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize RMSprop parameters\n",
        "    rmsweights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    rmsbias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,100):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"tanh\", \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, \"tanh\", A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                rmsweights[k] = beta * rmsweights[k] + (1 - beta) * (batch_w_delta[k] ** 2)\n",
        "                rmsbias[k] = beta * rmsbias[k] + (1 - beta) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                weights[k] -= (lr * batch_w_delta[k]) / (np.sqrt(rmsweights[k]) + eps)\n",
        "                bias[k] -= (lr * batch_b_delta[k]) / (np.sqrt(rmsbias[k]) + eps)\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "g17vSi0AT2iC"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr=0.001):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,50):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"tanh\", \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, \"tanh\", A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] -= (lr * m_weights_hat) / (np.sqrt(v_weights_hat) + eps)\n",
        "                bias[k] -= (lr * m_bias_hat) / (np.sqrt(v_bias_hat) + eps)\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "Dha24Dxz6Wiv"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr=0.001):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Nadam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for batch in range(0,20):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], \"tanh\", \"crossEntropy\", weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, \"tanh\", A)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] -= lr * (beta1 * m_weights_hat + (1 - beta1) * batch_w_delta[k]) / (np.sqrt(v_weights_hat) + eps)\n",
        "                bias[k] -= lr * (beta1 * m_bias_hat + (1 - beta1) * batch_b_delta[k]) / (np.sqrt(v_bias_hat) + eps)\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "kFFut1Z984-Z"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(nodesPerLayer,x_flatten_train,y_encoded):\n",
        "  weights = list()\n",
        "  bias = list()\n",
        "  for i in range(1,len(nodesPerLayer)):\n",
        "    w = np.random.randn(nodesPerLayer[i],nodesPerLayer[i-1])*0.1\n",
        "    b =  np.random.randn(nodesPerLayer[i],1)\n",
        "    weights.append(w)\n",
        "    bias.append(b)\n",
        "  Wdelta = list()\n",
        "  Bdelta = list()\n",
        "  for i in range(0,epochs):\n",
        "    Wdelta.clear()\n",
        "    Bdelta.clear()\n",
        "    print(\"Epoch:\" + str(i))\n",
        "    for j in range(0,len(y_train)):\n",
        "      A,B,C = forwardProp(x_flatten_train[j],\"sigmoid\",\"crossEntropy\",weights,bias)\n",
        "      CurrWdelta,CurrBdelta = backProp(y_encoded[j],C,B,weights,\"sigmoid\",A)\n",
        "      if( len(Wdelta) == 0):\n",
        "        Wdelta =  copy.deepcopy(CurrWdelta)\n",
        "        Bdelta = copy.deepcopy(CurrBdelta)\n",
        "      else:\n",
        "        for k in range(0,len(Wdelta)):\n",
        "          Wdelta[k] = Wdelta[k] + CurrWdelta[k]\n",
        "          Bdelta[k] = Bdelta[k] + CurrBdelta[k]\n",
        "      if(j%1000 == 0):\n",
        "        print(j/1000)\n",
        "    for k in range(0,len(weights)):\n",
        "      weights[k] = weights[k] - lr*Wdelta[k]\n",
        "      bias[k] = bias[k] - lr*Bdelta[k]\n",
        "  return weights,bias"
      ],
      "metadata": {
        "id": "XrUyqtbh8zlJ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(optimizer,x_train,y_train,nodesPerLayer):\n",
        "  FinalWeights = list()\n",
        "  FinalBias = list()\n",
        "  beta1 = 0.89\n",
        "  beta2 = 0.95\n",
        "  eps = 1e-6\n",
        "  batch_size = 200\n",
        "  if(optimizer == \"gradient_descent\"):\n",
        "    FinalWeights, FinalBias = gradient_descent(nodesPerLayer,x_train,y_train)\n",
        "  elif(optimizer == \"SGD\"):\n",
        "    FinalWeights, FinalBias = stochastic_gradient_descent(nodesPerLayer,x_train,y_train,batch_size)\n",
        "  elif(optimizer == \"momentumGD\"):\n",
        "    FinalWeights, FinalBias = momentum_gradient_descent(nodesPerLayer,x_train,y_train,0.6,batch_size)\n",
        "  elif(optimizer == \"nesterovGD\"):\n",
        "    FinalWeights, FinalBias = nesterov_gradient_descent(nodesPerLayer,x_train,y_train,0.6,batch_size)\n",
        "  elif(optimizer == \"rmsprop\"):\n",
        "    FinalWeights, FinalBias =rmsprop(nodesPerLayer,x_train,y_train,0.6,1e-6,2,batch_size,0.001)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    FinalWeights, FinalBias = adam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size,0.001)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    FinalWeights, FinalBias = nadam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size,0.001)\n",
        "  return FinalWeights,FinalBias"
      ],
      "metadata": {
        "id": "WLkW8suR9RCp"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = trainModel(\"momentumGD\",x_flatten_train,y_encoded,nodesPerLayer)"
      ],
      "metadata": {
        "id": "NiAR-tqwjweB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lU8sJHCOUnqN"
      },
      "outputs": [],
      "source": [
        "def testModel(weights,bias,x_test,y_test):\n",
        "  count = 0\n",
        "  for i in range(0,x_test.shape[0]):\n",
        "    A,B,C = forwardProp(x_test[i],\"tanh\",\"crossEntropy\",weights,bias)\n",
        "    if( y_test[i] == np.argmax(C)):\n",
        "      count+=1\n",
        "  print(\"Accuracy :\" + str((count/y_test.shape[0])*100) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "4rmZko4FZ94n"
      },
      "outputs": [],
      "source": [
        "x_flatten_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2],1)\n",
        "x_flatten_test = normalize_data(x_flatten_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V94N6E6_Z94o"
      },
      "outputs": [],
      "source": [
        "testModel(a,b,x_flatten_test,y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}