{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IASdQzqcXe89"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix as cnfm\n",
        "import itertools\n",
        "import math\n",
        "from sklearn.utils import shuffle\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(project='Assignment 1_Mnist', entity='CS23M028')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ei8zzE07CR9V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.datasets import fashion_mnist\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BJGaU2CWmYB4"
      },
      "outputs": [],
      "source": [
        "def normalize_data(x):\n",
        "  x_norm = x.astype('float32')\n",
        "  x_norm = x_norm / 255.0\n",
        "  return x_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pR0k8a2uXy3f"
      },
      "outputs": [],
      "source": [
        "def func(activation,a_k1):\n",
        "  a_k = np.clip(a_k1, -55, 55)\n",
        "  if(activation == \"tanh\"):\n",
        "    a_k = np.tanh(a_k)\n",
        "  elif(activation == \"sigmoid\"):\n",
        "    a_k = 1/(1 + np.exp(-1*a_k))\n",
        "  else:\n",
        "    a_k = np.maximum(0,a_k)\n",
        "  return a_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uas-jhRzYIOm"
      },
      "outputs": [],
      "source": [
        "def derivativeFun(activation,a_k1):\n",
        "  a_k = np.clip(a_k1, -55, 55)\n",
        "  activationResult = func(activation,a_k)\n",
        "  if(activation == \"tanh\"):\n",
        "    activationResult = 1 - (activationResult**2)\n",
        "  elif(activation == \"sigmoid\"):\n",
        "    activationResult = activationResult - (activationResult**2)\n",
        "  else:\n",
        "    activationResult = np.where(a_k > 0, 1, 0)\n",
        "  return activationResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u3_w6ojmaD52"
      },
      "outputs": [],
      "source": [
        "def decision(a_k):\n",
        "  a_k = np.exp(a_k - np.max(a_k))\n",
        "  a_k = a_k / sum(a_k)\n",
        "  return a_k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ErGdug_sVCXM"
      },
      "outputs": [],
      "source": [
        "def forwardProp(inputX,activation,weights,bias):\n",
        "  h_k = inputX\n",
        "  PreActivations = list()\n",
        "  PostActivations = list()\n",
        "  PostActivations.append(h_k)\n",
        "  layers = len(weights) - 1\n",
        "  for k in range(0,layers):\n",
        "    a_k = bias[k] + np.dot(weights[k],h_k)\n",
        "    PreActivations.append(a_k)\n",
        "    h_k = func(activation,a_k)\n",
        "    PostActivations.append(h_k)\n",
        "  a_k = bias[layers] + np.matmul(weights[layers],h_k)\n",
        "  PreActivations.append(a_k)\n",
        "  yPred = decision(a_k)\n",
        "  return PreActivations,PostActivations,yPred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d7ZFrgcUSaHK"
      },
      "outputs": [],
      "source": [
        "def backProp(real, pred, h_k, weights, activation, PreActivations,lossFunction):\n",
        "    a_l_L_theta = pred - real\n",
        "    if lossFunction == \"cross_entropy\":\n",
        "      a_l_L_theta = pred - real\n",
        "    elif lossFunction == \"mean_squared_error\":\n",
        "      a_l_L_theta = np.multiply(np.multiply((pred - real), pred), (1 - pred))\n",
        "\n",
        "    currentActivationGradient = a_l_L_theta\n",
        "    WeightGradients = []\n",
        "    biasGradients = []\n",
        "    layers = len(weights) - 1\n",
        "\n",
        "    for i in range(layers, -1, -1):\n",
        "        W_i_L_theta = currentActivationGradient*np.transpose(h_k[i])\n",
        "        WeightGradients.insert(0, W_i_L_theta)\n",
        "        b_i_L_theta = np.sum(currentActivationGradient, axis=0, keepdims=True)\n",
        "        biasGradients.insert(0, b_i_L_theta)\n",
        "\n",
        "        if i > 0:\n",
        "            h_i_prev_L_theta = np.matmul(weights[i].T, currentActivationGradient)\n",
        "            currentActivationGradient = h_i_prev_L_theta * derivativeFun(activation, PreActivations[i - 1])\n",
        "\n",
        "    return WeightGradients, biasGradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D26Wazq9kcLV"
      },
      "outputs": [],
      "source": [
        "def randomizer(dim1,dim2,init_weight):\n",
        "  std_dev = 0.1\n",
        "  if(init_weight == \"Xavier\"):\n",
        "    variance = 2.0 / (dim1 + dim2)\n",
        "    std_dev = np.sqrt(variance)\n",
        "  return std_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sXsdaBX9JjkV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stochastic_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            if(batch % (num_batches/10) == 0):\n",
        "              print(\"batch: \" + str(batch))\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc,weights, bias)\n",
        "                Wdelta, Bdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += Wdelta[k]\n",
        "                    batch_Bdelta[k] += Bdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - lr * (batch_Wdelta[k] / batch_size)\n",
        "                bias[k] -= lr * (batch_Bdelta[k] / batch_size)\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi4rekLRNIH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def momentum_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, gamma, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    Wdelta = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    Bdelta = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            print(batch)\n",
        "            if(batch % (num_batches/10) == 0):\n",
        "              print(\"batch :\" + str(batch))\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_Wdelta = [np.zeros_like(w) for w in weights]\n",
        "            batch_Bdelta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights,activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(batch_Wdelta)):\n",
        "                    batch_Wdelta[k] += CurrWdelta[k]\n",
        "                    batch_Bdelta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(weights)):\n",
        "                Wdelta[k] = gamma * Wdelta[k] + lr * batch_Wdelta[k] / batch_size\n",
        "                Bdelta[k] = gamma * Bdelta[k] + lr * batch_Bdelta[k] / batch_size\n",
        "\n",
        "                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) -Wdelta[k]\n",
        "                bias[k] -= Bdelta[k]\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dUnrGa1nBWyx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nesterov_gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,gamma, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            lookahead_weights = [w - gamma * dw for w, dw in zip(weights, weights)]\n",
        "            lookahead_bias = [b - gamma * db for b, db in zip(bias, bias)]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, lookahead_weights, lookahead_bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, lookahead_weights, activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(weights)):\n",
        "                    weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - lr * CurrWdelta[k]\n",
        "                    bias[k] -= lr * CurrBdelta[k]\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g17vSi0AT2iC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rmsprop(nodesPerLayer, x_flatten_train, y_encoded, beta, eps, epochs, batch_size, lr,activationFunc,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    rmsweights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    rmsbias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc,  weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                rmsweights[k] = beta * rmsweights[k] + (1 - beta) * (batch_w_delta[k] ** 2)\n",
        "                rmsbias[k] = beta * rmsbias[k] + (1 - beta) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                weights[k] =((1 - (lr * lambda_reg / batch_size)) * weights[k]) -( (lr * batch_w_delta[k]) / (np.sqrt(rmsweights[k]) + eps))\n",
        "                bias[k] -= (lr * batch_b_delta[k]) / (np.sqrt(rmsbias[k]) + eps)\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dha24Dxz6Wiv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def adam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc,  weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - ( (lr * m_weights_hat) / (np.sqrt(v_weights_hat) + eps))\n",
        "                bias[k] -= (lr * m_bias_hat) / (np.sqrt(v_bias_hat) + eps)\n",
        "\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kFFut1Z984-Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nadam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n",
        "    # Initialize weights and biases\n",
        "    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n",
        "    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    # Initialize Nadam parameters\n",
        "    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n",
        "    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n",
        "\n",
        "    num_batches = len(x_flatten_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(0,num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = (batch + 1) * batch_size\n",
        "            batch_x = x_flatten_train[start:end]\n",
        "            batch_y = y_encoded[start:end]\n",
        "\n",
        "            batch_w_delta = [np.zeros_like(w) for w in weights]\n",
        "            batch_b_delta = [np.zeros_like(b) for b in bias]\n",
        "\n",
        "            for j in range(len(batch_x)):\n",
        "                A, B, C = forwardProp(batch_x[j], activationFunc, weights, bias)\n",
        "                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n",
        "\n",
        "                for k in range(len(CurrWdelta)):\n",
        "                    batch_w_delta[k] += CurrWdelta[k]\n",
        "                    batch_b_delta[k] += CurrBdelta[k]\n",
        "\n",
        "            for k in range(len(batch_w_delta)):\n",
        "                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n",
        "                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n",
        "                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n",
        "                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n",
        "\n",
        "                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n",
        "                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                weights[k] =((1 - (lr * lambda_reg / batch_size)) * weights[k]) - ( lr * (beta1 * m_weights_hat + (1 - beta1) * batch_w_delta[k]) / (np.sqrt(v_weights_hat) + eps))\n",
        "                bias[k] -= lr * (beta1 * m_bias_hat + (1 - beta1) * batch_b_delta[k]) / (np.sqrt(v_bias_hat) + eps)\n",
        "        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n",
        "        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n",
        "        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n",
        "\n",
        "    return weights, bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XrUyqtbh8zlJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,activationFunc,epochs,lr):\n",
        "  weights = list()\n",
        "  bias = list()\n",
        "  w = np.random.randn(nodesPerLayer[i],nodesPerLayer[i-1])*0.1\n",
        "  b =  np.random.randn(nodesPerLayer[i],1)\n",
        "  weights.append(w)\n",
        "  bias.append(b)\n",
        "  Wdelta = list()\n",
        "  Bdelta = list()\n",
        "  for i in range(0,epochs):\n",
        "    Wdelta.clear()\n",
        "    Bdelta.clear()\n",
        "    for j in range(0,len(y_encoded)):\n",
        "      A,B,C = forwardProp(x_flatten_train[j],activationFunc,weights,bias)\n",
        "      CurrWdelta,CurrBdelta = backProp(y_encoded[j],C,B,weights,activationFunc,A)\n",
        "      if( len(Wdelta) == 0):\n",
        "        Wdelta =  copy.deepcopy(CurrWdelta)\n",
        "        Bdelta = copy.deepcopy(CurrBdelta)\n",
        "      else:\n",
        "        for k in range(0,len(Wdelta)):\n",
        "          Wdelta[k] = Wdelta[k] + CurrWdelta[k]\n",
        "          Bdelta[k] = Bdelta[k] + CurrBdelta[k]\n",
        "      if(j%1000 == 0):\n",
        "        print(j/1000)\n",
        "    for k in range(0,len(weights)):\n",
        "      weights[k] = weights[k] - lr*Wdelta[k]\n",
        "      bias[k] = bias[k] - lr*Bdelta[k]\n",
        "  return weights,bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WLkW8suR9RCp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def executeTraining(config,x_train,y_train,x_flatten_test,y_flatten_test,x_val,y_val):\n",
        "  FinalWeights = list()\n",
        "  FinalBias = list()\n",
        "  beta1 = 0.8\n",
        "  beta2 = 0.9\n",
        "  eps = 1e-6\n",
        "  batch_size = config.batch_size\n",
        "  layers = config.number_hidden\n",
        "  lr = config.learning_rate\n",
        "  epochs = config.epochs\n",
        "  activationFunc = config.activation\n",
        "  optimizer = config.optimizer\n",
        "  weightInit = config.weight_init\n",
        "  lambda_reg = config.weight_decay\n",
        "  nodesPerLayer = list()\n",
        "  nodesPerLayer.append(784)\n",
        "  for i in range(0,layers):\n",
        "    nodesPerLayer.append(config.number_hidden)\n",
        "  nodesPerLayer.append(10)\n",
        "\n",
        "  lossFunction = \"cross_entropy\"\n",
        "  gamma = 0.00001\n",
        "  betarms = 0.001\n",
        "  if(optimizer == \"gradient_descent\"):\n",
        "    FinalWeights, FinalBias = gradient_descent(nodesPerLayer,x_train,y_train,activationFunc,epochs,lr)\n",
        "  elif(optimizer == \"sgd\"):\n",
        "    FinalWeights, FinalBias = stochastic_gradient_descent(nodesPerLayer,x_train,y_train,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  elif(optimizer == \"momentum\"):\n",
        "    FinalWeights, FinalBias = momentum_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  elif(optimizer == \"nag\"):\n",
        "    FinalWeights, FinalBias = nesterov_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  elif(optimizer == \"rmsprop\"):\n",
        "    FinalWeights, FinalBias =rmsprop(nodesPerLayer,x_train,y_train,betarms,eps,epochs,batch_size,lr,activationFunc,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    FinalWeights, FinalBias = adam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    FinalWeights, FinalBias = nadam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n",
        "  return FinalWeights,FinalBias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lU8sJHCOUnqN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def testModel(weights,bias,x_test,y_test,activationFun,lossFunction):\n",
        "  count = 0\n",
        "  loss = 0.0\n",
        "\n",
        "  for i in range(0,x_test.shape[0]):\n",
        "    A,B,C = forwardProp(x_test[i],activationFun,weights,bias)\n",
        "    if( y_test[i]==np.argmax(C)):\n",
        "      count+=1\n",
        "\n",
        "    if(lossFunction == \"mean_squared_error\"):\n",
        "      loss += (np.argmax(C) -  y_test[i])**2\n",
        "    else:\n",
        "      loss += -np.log(C)[y_test[i]][0]\n",
        "\n",
        "  loss /= y_test.shape[0]\n",
        "  acc = (count/y_test.shape[0])\n",
        "  return acc,loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def import_data(dataset):\n",
        "  if(dataset == \"mnist\"):\n",
        "   return mnist.load_data()\n",
        "  else:\n",
        "   return fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "id": "h9vEHIW0nDsa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'name':\"my-sweep\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5, 10] #number of epochs\n",
        "        },\n",
        "        'number_hidden': {\n",
        "            'values': [3, 4, 5] #number of hidden layers\n",
        "        },\n",
        "        'hidden_inputsize': {\n",
        "            'values':[32, 64, 128] #size of every hidden layer\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005,  0.5] #L2 regularisation\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['momentum']\n",
        "        }, #'nag', 'rmsprop', 'adam', 'nadam','sgd'\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values':['random','Xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"CS23M028\", project=\"Assignment 1_1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFvl6_L2n8tl",
        "outputId": "2ed10b14-74eb-4dc3-de63-772500eee5cd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: o2eeqi8d\n",
            "Sweep URL: https://wandb.ai/cs23m028/Assignment%201_1/sweeps/o2eeqi8d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8nEmrnfUFPAn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(dataset):\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = import_data(dataset)\n",
        "  x_train, y_train = shuffle(x_train, y_train)\n",
        "  val_size = int(x_train.shape[0] * 0.1)\n",
        "\n",
        "  y_val = y_train[:val_size]\n",
        "  y_new_train = y_train[val_size:]\n",
        "\n",
        "  x_flatten_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)\n",
        "  x_flatten_train = normalize_data(x_flatten_train)\n",
        "\n",
        "  y_encoded = np.zeros((y_new_train.shape[0], 10))\n",
        "  y_encoded[np.arange(y_new_train.shape[0]), y_new_train] = 1\n",
        "  y_new_train = y_encoded.reshape(y_new_train.shape[0],10,1)\n",
        "\n",
        "\n",
        "  x_flatten_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2],1)\n",
        "  x_flatten_test = normalize_data(x_flatten_test)\n",
        "\n",
        "\n",
        "  x_val = x_flatten_train[:val_size]\n",
        "\n",
        "\n",
        "  x_new_train = x_flatten_train[val_size:]\n",
        "\n",
        "\n",
        "  return x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_diffClasses():\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  names = [\"T-shirt\",\"Trouser\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "  variousSamples = list()\n",
        "  classes = set(y_train)\n",
        "  for i in classes:\n",
        "    ind = np.where(y_train == i)[0][0]\n",
        "    variousSamples.append(wandb.Image(x_train[ind],caption = names[i]))\n",
        "\n",
        "  wandb.log({\"examples\": variousSamples})\n"
      ],
      "metadata": {
        "id": "99G2i7vUnN7r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def GenConfusion(weights,bias,x_test,y_test,activationFun):\n",
        "  prediction = list()\n",
        "  names = [\"T-shirt\",\"Trouser\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "  for i in range(0,x_test.shape[0]):\n",
        "    A,B,C = forwardProp(x_test[i],activationFun,\"crossEntropy\",weights,bias)\n",
        "    prediction.append(np.argmax(C))\n",
        "  wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(y_true=y_test, preds=prediction,class_names=names)})\n",
        "  return\n"
      ],
      "metadata": {
        "id": "SJnPdwO6nSnp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiAR-tqwjweB"
      },
      "outputs": [],
      "source": [
        "def fitModel():\n",
        "  with wandb.init() as run:\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"hidden_\" + str(config.hidden_inputsize)+\"_batchSize_\"+str(config.batch_size)+\"_acc_\"+ config.activation\n",
        "    np.random.seed(1)\n",
        "\n",
        "    x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_flatten_test = load_data(\"fashion_mnist\")\n",
        "    trainedWeights,trainedBias = executeTraining(config,x_new_train,y_new_train,x_flatten_test,y_flatten_test,x_val,y_val)\n",
        "\n",
        "wandb.agent(sweep_id,fitModel,entity=\"CS23M028\", project=\"Assignment 1_1\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}